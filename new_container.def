Bootstrap: docker
From: nvidia/cuda:12.1.0-cudnn8-devel-ubuntu22.04

%files
    # Copy requirements.txt into the container during build
    requirements.txt /opt/requirements.txt

%post
    echo "Starting container build..."
    
    # Update and install system dependencies
    apt-get update && apt-get install -y \
        python3.10 \
        python3-pip \
        python3-dev \
        git \
        wget \
        curl \
        build-essential \
        software-properties-common \
        && rm -rf /var/lib/apt/lists/*

    # Create symbolic links for python
    update-alternatives --install /usr/bin/python python /usr/bin/python3.10 1
    update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.10 1
    ln -sf /usr/bin/python3.10 /usr/local/bin/python
    ln -sf /usr/bin/python3.10 /usr/local/bin/python3

    # Upgrade pip
    python3 -m pip install --upgrade pip setuptools wheel

    # Install PyTorch first with CUDA 12.1 support from PyTorch index
    echo "Installing PyTorch with CUDA 12.1 support..."
    pip install --no-cache-dir torch==2.3.1 torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121

    # Install other packages from requirements.txt, excluding torch and CUDA packages
    echo "Installing other Python packages..."
    grep -v "^torch==" /opt/requirements.txt | \
    grep -v "^triton==" | \
    grep -v "^nvidia-" | \
    pip install --no-cache-dir -r /dev/stdin || true

    # Verify critical installations
    echo "Verifying installations..."
    python -c "import torch; print(f'PyTorch {torch.__version__} installed')" || echo "WARNING: PyTorch verification failed"
    python -c "import torch; print(f'CUDA available: {torch.cuda.is_available()}')" || echo "WARNING: CUDA check failed (this is OK during build)"
    python -c "import transformers; print(f'Transformers {transformers.__version__} installed')" || echo "WARNING: Transformers verification failed"
    
    # Verify python command works
    python --version
    python3 --version
    
    # Clean up to reduce image size
    apt-get clean
    rm -rf ~/.cache/pip
    rm -rf /var/lib/apt/lists/*

    echo "Container build complete!"

%environment
    # Set environment variables
    export LC_ALL=C
    export PYTHONPATH=/workspace:$PYTHONPATH
    export CUDA_HOME=/usr/local/cuda
    export TOKENIZERS_PARALLELISM=false
    export HF_HOME=/workspace/.cache/huggingface
    export TRANSFORMERS_CACHE=/workspace/.cache/huggingface
    export PATH=/usr/local/bin:$PATH

%runscript
    # Default behavior when running the container
    if command -v python &> /dev/null; then
        exec python "$@"
    else
        exec python3 "$@"
    fi

%startscript
    echo "CSDS447 Layer-Wise CHAIR Container Ready"

%labels
    Author CSDS447_Team
    Version 1.3
    Description Container for Layer-Wise CHAIR Hallucination Detection with LLaMA models
    GPU_Required Yes
    CUDA_Version 12.1
    PyTorch_Version 2.3.1

%help
    ===================================================================
    CSDS447 Layer-Wise CHAIR Hallucination Detection Container
    ===================================================================
    
    This container provides the complete environment for running 
    layer-wise hallucination detection experiments with LLaMA models.
    
    REQUIREMENTS:
    - NVIDIA GPU with CUDA support
    - Singularity/Apptainer 3.0+
    - GPU drivers compatible with CUDA 12.1
    
    INSTALLED VERSIONS:
    - Python 3.10
    - PyTorch 2.3.1 with CUDA 12.1
    - Transformers (latest compatible)
    
    USAGE EXAMPLES:
    
    1. Interactive shell:
       singularity shell --nv --bind $PWD:/workspace --pwd /workspace singularity.sif
    
    2. Run setup:
       singularity exec --nv --bind $PWD:/workspace --pwd /workspace singularity.sif make setup
    
    3. Full LR pipeline:
       singularity exec --nv --bind $PWD:/workspace --pwd /workspace singularity.sif make full_run_lr
    
    4. Full NN pipeline:
       singularity exec --nv --bind $PWD:/workspace --pwd /workspace singularity.sif make full_run_nn
    
    5. Custom Python script:
       singularity exec --nv --bind $PWD:/workspace --pwd /workspace singularity.sif python your_script.py
    
    NOTES:
    - Always use --nv flag for GPU access
    - Use --bind to mount your project directory
    - Use --pwd to set working directory inside container
    - Both 'python' and 'python3' commands are available
    
    For more information, see README.md
    ===================================================================
